{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867a45f4-5bb6-4db5-a185-4be314e56605",
   "metadata": {},
   "source": [
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c21e4-5f03-4051-82e4-a9940623d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e580d34b-cf26-465c-93dc-f93d2d523377",
   "metadata": {},
   "source": [
    "Data preprocessing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d322083c-5315-45b4-9b40-25c5db65139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "data = pd.read_csv(\"C:/Users/Dell/Desktop/E-commerce03.csv\")\n",
    "\n",
    "# Fill missing values with median for numerical columns\n",
    "data['Annual Income'] = data['Annual Income'].fillna(data['Annual Income'].median())\n",
    "data['Age'] = data['Age'].fillna(data['Age'].median())\n",
    "data['Time on Site'] = data['Time on Site'].fillna(data['Time on Site'].median())\n",
    "\n",
    "# Fill missing values with mode for categorical column\n",
    "data['Location'] = data['Location'].fillna(data['Location'].mode()[0])\n",
    "\n",
    "# Filter users within reasonable age range\n",
    "data = data[(data['Age'] >= 18) & (data['Age'] <= 100)]\n",
    "\n",
    "# Bin Annual Income into Income Tier\n",
    "data['Income Tier'] = pd.cut(data['Annual Income'], bins=[0, 50000, 80000, 110000, np.inf],\n",
    "                             labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Define region mapping\n",
    "region_mapping = {\n",
    "    'City P': 'Region 1', 'City N': 'Region 2', 'City V': 'Region 3', 'City W': 'Region 4',\n",
    "    'City Q': 'Region 6', 'City F': 'Region 7', 'City B': 'Region 8',\n",
    "    'City A': 'Region 1', 'City T': 'Region 2', 'City U': 'Region 3', 'City Z': 'Region 4',\n",
    "    'City X': 'Region 5', 'City Y': 'Region 6', 'City L': 'Region 7', 'City M': 'Region 8',\n",
    "    'City C': 'Region 1', 'City D': 'Region 2', 'City S': 'Region 3', 'City J': 'Region 4',\n",
    "    'City O': 'Region 5', 'City E': 'Region 8'\n",
    "}\n",
    "\n",
    "# Map city locations to regions\n",
    "data['Region'] = data['Location'].map(region_mapping)\n",
    "\n",
    "# Bin Time on Site into Time on Site Tier\n",
    "data['Time on Site Tier'] = pd.cut(data['Time on Site'], bins=[0, 100, 200, 300, np.inf],\n",
    "                                   labels=['Short', 'Medium', 'Long', 'Very Long'])\n",
    "\n",
    "# Save processed data to new CSV file\n",
    "try:\n",
    "    data.to_csv(\"C:/Users/Dell/Desktop/E-commerce04.csv\", index=False)\n",
    "    print(\"Data successfully saved to file.\")\n",
    "except PermissionError:\n",
    "    print(\"Permission denied to write file. Please ensure the file is not being used by another program and you have sufficient permissions.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2717a5-23f7-4389-90ae-8f24e2883fdc",
   "metadata": {},
   "source": [
    "KNN Recommendation part -Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb4237-0f15-49ac-a460-cd89d7a82f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv(\"C:/Users/Dell/Desktop/E-commerce04.csv\")\n",
    "\n",
    "# Extract user IDs and purchase histories\n",
    "user_ids = data['Customer ID'].tolist()\n",
    "purchase_history = data['Purchase History'].tolist()\n",
    "\n",
    "# Collect all product categories\n",
    "all_product_categories = set()\n",
    "for history in purchase_history:\n",
    "    purchases = eval(history)\n",
    "    for purchase in purchases:\n",
    "        all_product_categories.add(purchase['Product Category'])\n",
    "\n",
    "# Create product-user interaction matrix\n",
    "interaction_matrix = pd.DataFrame(index=user_ids, columns=list(all_product_categories), dtype=float)\n",
    "interaction_matrix = interaction_matrix.fillna(0)\n",
    "\n",
    "# Fill the matrix elements with ratings\n",
    "for i, user_id in enumerate(user_ids):\n",
    "    purchases = eval(purchase_history[i])\n",
    "    for purchase in purchases:\n",
    "        category = purchase['Product Category']\n",
    "        rating = purchase['Product Review']['Rating']\n",
    "        if category in interaction_matrix.columns:\n",
    "            interaction_matrix.loc[user_id, category] = rating\n",
    "\n",
    "# Split into training set and validation set\n",
    "train_data, validation_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Randomly remove 50% of the purchase records for validation set users\n",
    "for index, row in validation_data.iterrows():\n",
    "    purchases = eval(row['Purchase History'])\n",
    "    if purchases:  # Ensure purchase records are not empty\n",
    "        # Randomly remove 50% of purchase records\n",
    "        purchases_to_keep = np.random.choice(purchases, size=int(len(purchases) * 0.5), replace=False)\n",
    "        validation_data.at[index, 'Purchase History'] = str(purchases_to_keep.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a229f1-21ed-4097-9c57-dca8999ca672",
   "metadata": {},
   "source": [
    "KNN Recommendation part - Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa8b50-da19-4d1f-b539-6a947f09834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use KNN algorithm to find similar users\n",
    "def find_similar_users(user_id, interaction_matrix, n_neighbors=5):\n",
    "    # Extract the rating data of the user from the interaction matrix\n",
    "    user_ratings = interaction_matrix.loc[user_id].values.reshape(1, -1)\n",
    "\n",
    "    # Use KNN algorithm to find similar users\n",
    "    knn = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine')\n",
    "    knn.fit(interaction_matrix.fillna(0).values)\n",
    "\n",
    "    distances, indices = knn.kneighbors(user_ratings)\n",
    "\n",
    "    # Get the indices of similar users\n",
    "    similar_user_indices = indices[0][1:]  # Exclude the user themselves\n",
    "\n",
    "    # Map indices back to user IDs\n",
    "    similar_users = [interaction_matrix.index[i] for i in similar_user_indices]\n",
    "\n",
    "    return similar_users\n",
    "\n",
    "# Generate recommendation list for validation set users\n",
    "def generate_recommendations(validation_data, interaction_matrix, top_n=3):\n",
    "    recommendations = {}\n",
    "    for user_id in validation_data['Customer ID']:\n",
    "        similar_users = find_similar_users(user_id, interaction_matrix)\n",
    "\n",
    "        # Collect purchase records of similar users\n",
    "        recommended_items = {}\n",
    "        for similar_user in similar_users:\n",
    "            if similar_user in interaction_matrix.index:\n",
    "                similar_user_ratings = interaction_matrix.loc[similar_user].dropna()\n",
    "                for item in similar_user_ratings.index:\n",
    "                    if pd.notna(similar_user_ratings[item]):\n",
    "                        if item in recommended_items:\n",
    "                            recommended_items[item] += 1\n",
    "                        else:\n",
    "                            recommended_items[item] = 1\n",
    "\n",
    "        # Get items the user has already purchased (after removal of purchase records)\n",
    "        purchases = eval(validation_data[validation_data['Customer ID'] == user_id]['Purchase History'].values[0])\n",
    "        purchased_items = set(purchase['Product Category'] for purchase in purchases)\n",
    "\n",
    "        # Remove items the user has already purchased\n",
    "        recommended_items = {item: count for item, count in recommended_items.items() if item not in purchased_items}\n",
    "\n",
    "        # Sort recommended items by frequency in descending order\n",
    "        sorted_recommended_items = sorted(recommended_items.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Extract item names and limit the size of the recommendation list\n",
    "        sorted_recommended_items = [item for item, count in sorted_recommended_items][:top_n]\n",
    "\n",
    "        recommendations[user_id] = sorted_recommended_items\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "def calculate_metrics(recommendations, validation_data):\n",
    "    precision_total = 0\n",
    "    recall_total = 0\n",
    "    total_users = len(recommendations)\n",
    "\n",
    "    for user_id in recommendations:\n",
    "        recommended_items = set(recommendations[user_id])\n",
    "        actual_items = set()\n",
    "\n",
    "        # Get the user's true purchase records (complete purchase history)\n",
    "        original_purchases = eval(data[data['Customer ID'] == user_id]['Purchase History'].values[0])\n",
    "        for purchase in original_purchases:\n",
    "            actual_items.add(purchase['Product Category'])\n",
    "\n",
    "        # Calculate intersection\n",
    "        intersection = recommended_items.intersection(actual_items)\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        precision = len(intersection) / len(recommended_items) if len(recommended_items) > 0 else 0\n",
    "        recall = len(intersection) / len(actual_items) if len(actual_items) > 0 else 0\n",
    "\n",
    "        precision_total += precision\n",
    "        recall_total += recall\n",
    "\n",
    "    # Calculate average precision and recall\n",
    "    avg_precision = precision_total / total_users\n",
    "    avg_recall = recall_total / total_users\n",
    "\n",
    "    # Calculate F1-score\n",
    "    f1 = (2 * avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "\n",
    "    return avg_precision, avg_recall, f1\n",
    "\n",
    "# Generate user similarity matrix\n",
    "def generate_user_similarity_matrix(interaction_matrix):\n",
    "    # Fill missing values with 0\n",
    "    filled_matrix = interaction_matrix.fillna(0)\n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = cosine_similarity(filled_matrix)\n",
    "    # Convert to DataFrame\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=interaction_matrix.index, columns=interaction_matrix.index)\n",
    "    return similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ca1bf6-88a1-41ff-9285-3ae04a8ca73a",
   "metadata": {},
   "source": [
    "KNN Recommendation part -Generate recommendation list and calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44545898-dbf3-4ff6-9735-0e8587e54bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendation list\n",
    "recommendations = generate_recommendations(validation_data, interaction_matrix)\n",
    "\n",
    "# Print debug information\n",
    "print(\"Example of recommendation list:\")\n",
    "for user_id, items in list(recommendations.items())[:5]:\n",
    "    print(f\"Recommendation list for user {user_id}: {items}\")\n",
    "\n",
    "# Calculate metrics\n",
    "precision, recall, f1 = calculate_metrics(recommendations, validation_data)\n",
    "\n",
    "# Output results\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ada95-66f6-45c4-bbbe-b26f10277223",
   "metadata": {},
   "source": [
    "KNN recommendation part - Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a261e-d62e-4430-adab-f398b32d1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize user-product category interaction matrix\n",
    "def visualize_interaction_matrix(interaction_matrix):\n",
    "    # Use PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=2)\n",
    "    interaction_matrix_filled = interaction_matrix.fillna(0)\n",
    "    user_embeddings = pca.fit_transform(interaction_matrix_filled)\n",
    "\n",
    "    # Plot scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(user_embeddings[:, 0], user_embeddings[:, 1], alpha=0.5)\n",
    "    plt.title('User Embeddings using PCA')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize recommendation accuracy\n",
    "def visualize_recommendation_accuracy(recommendations, validation_data, interaction_matrix, data):\n",
    "    # Collect actual purchased categories and recommended categories\n",
    "    actual_categories = []\n",
    "    recommended_categories = []\n",
    "\n",
    "    for user_id in recommendations:\n",
    "        # Get actual purchase records\n",
    "        actual_purchases = eval(data[data['Customer ID'] == user_id]['Purchase History'].values[0])\n",
    "        actual_categories.extend([purchase['Product Category'] for purchase in actual_purchases])\n",
    "\n",
    "        # Get recommended records\n",
    "        recommended_categories.extend(recommendations[user_id])\n",
    "\n",
    "    # Calculate frequency\n",
    "    actual_counts = pd.Series(actual_categories).value_counts().sort_index()\n",
    "    recommended_counts = pd.Series(recommended_categories).value_counts().sort_index()\n",
    "\n",
    "    # Plot comparison chart\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Actual purchase category distribution\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    actual_counts.plot(kind='bar', color='skyblue', ax=ax1)\n",
    "    ax1.set_title('Actual Purchase Distribution')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "\n",
    "    # Recommended category distribution\n",
    "    ax2 = plt.subplot(2, 1, 2)\n",
    "    recommended_counts.plot(kind='bar', color='orange', ax=ax2)\n",
    "    ax2.set_title('Recommended Distribution')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Perform visualization\n",
    "visualize_interaction_matrix(interaction_matrix)\n",
    "visualize_recommendation_accuracy(recommendations, validation_data, interaction_matrix, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bade2b38-4233-4b4c-9793-eebf784265cf",
   "metadata": {},
   "source": [
    "Association Analysis part - Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20fb32d-6d0b-4999-a5f8-16a6134f52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Dell\\Desktop\\E-commerce04.csv\")\n",
    "\n",
    "# Define age grouping function\n",
    "def get_age_group(age):\n",
    "    if age < 30:\n",
    "        return '<30岁'\n",
    "    elif 30 <= age <= 50:\n",
    "        return '30-50岁'\n",
    "    else:\n",
    "        return '>50岁'\n",
    "\n",
    "# Build the transaction dataset\n",
    "transactions = []\n",
    "for idx, row in df.iterrows():\n",
    "    # Extract user attributes\n",
    "    gender = row['Gender']\n",
    "    income_tier = row['Income Tier']\n",
    "    region = row['Region']\n",
    "    time_on_site_tier = row['Time on Site Tier']\n",
    "    age_group = get_age_group(row['Age'])\n",
    "\n",
    "    # Parse purchase history and remove duplicates\n",
    "    purchase_cats = list({item['Product Category'] for item in json.loads(row['Purchase History'])})\n",
    "\n",
    "    # Combine attribute items and purchase category items\n",
    "    transaction = [\n",
    "        gender, income_tier, region, time_on_site_tier, age_group,\n",
    "        *purchase_cats  # Unfold purchase category items\n",
    "    ]\n",
    "    # Clean data to ensure all items are of string type\n",
    "    transaction = [str(item) for item in transaction if pd.notna(item)]\n",
    "    transactions.append(transaction)\n",
    "\n",
    "# Generate a list of user IDs (assuming user IDs are unique)\n",
    "user_ids = df['Customer ID'].tolist()\n",
    "\n",
    "# Split the dataset (80% training, 20% validation)\n",
    "train_users, val_users = train_test_split(user_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Filter transactions for training and validation sets\n",
    "train_transactions = [t for u, t in zip(user_ids, transactions) if u in train_users]\n",
    "val_transactions = [t for u, t in zip(user_ids, transactions) if u in val_users]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758cc76f-9e1d-4941-a07b-6dbe5dce09d0",
   "metadata": {},
   "source": [
    "Association analysis part -Generating frequent itemsets and rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7229249-b731-4e78-b55c-bd6af747e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transaction encoder\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit_transform(train_transactions)\n",
    "train_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Generate frequent itemsets (minimum support set to 5%)\n",
    "frequent_itemsets = apriori(\n",
    "    train_df,\n",
    "    min_support=0.05,\n",
    "    use_colnames=True,\n",
    "    max_len=5  # Limit itemset length (total items in antecedent and consequent)\n",
    ")\n",
    "\n",
    "# Calculate rule metrics (support, confidence, lift)\n",
    "rules = association_rules(\n",
    "    frequent_itemsets,\n",
    "    metric=\"confidence\",\n",
    "    min_threshold=0.5  # Minimum confidence 50%\n",
    ")\n",
    "\n",
    "# Filter rules: consequent is a single purchase category, antecedent contains at least one attribute item\n",
    "attribute_items = {'Male', 'Female', '<30岁', '30-50岁', '>50岁',\n",
    "                   'Low', 'Medium', 'High', 'Very High',\n",
    "                   'Region 1', 'Region 2', 'Region 3', 'Region 4',\n",
    "                   'Region 5', 'Region 6', 'Region 7', 'Region 8',\n",
    "                   'Short', 'Medium', 'Long', 'Very Long'}\n",
    "\n",
    "valid_rules = rules[\n",
    "    (rules['consequents'].apply(lambda x: len(x) == 1)) &  # Consequent is a single category\n",
    "    (rules['antecedents'].apply(lambda x: not x.isdisjoint(attribute_items)))  # Antecedent contains attribute items\n",
    "    ].copy()\n",
    "\n",
    "# Correctly calculate lift\n",
    "valid_rules['Lift'] = valid_rules['confidence'] / valid_rules['consequent support']\n",
    "\n",
    "# Filter valid rules (lift > 1)\n",
    "final_rules = valid_rules[valid_rules['Lift'] > 1].sort_values(by='confidence', ascending=False)\n",
    "\n",
    "print(f\"Total transactions (training set): {len(train_transactions)}\")\n",
    "print(f\"Number of generated rules: {len(final_rules)}\")\n",
    "print(\"\\nTop 5 effective rules:\")\n",
    "print(final_rules[['antecedents', 'consequents', 'Lift']].head())\n",
    "\n",
    "# Group by core attributes (e.g., income tier) to reduce search space\n",
    "for tier in df['Income Tier'].unique():\n",
    "    tier_transactions = [t for t in train_transactions if tier in t]\n",
    "    # Apply Apriori algorithm separately for each group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a273632-5ab0-4c06-8c6f-90921aea3d99",
   "metadata": {},
   "source": [
    "Association analysis part -validation set operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c241c-6343-4feb-a621-b92468b280fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the validation set\n",
    "val_te_ary = te.transform(val_transactions)  # Use the encoder from the training set\n",
    "val_df = pd.DataFrame(val_te_ary, columns=te.columns_)\n",
    "\n",
    "# Calculate rule coverage on the validation set\n",
    "def rule_coverage(rule, val_df):\n",
    "    antecedent = set(rule['antecedents'])\n",
    "    consequent = set(rule['consequents'])\n",
    "    antecedent_mask = val_df[list(antecedent)].all(axis=1)\n",
    "    return val_df[antecedent_mask][list(consequent)].any(axis=1).mean()\n",
    "\n",
    "final_rules['validation_coverage'] = final_rules.apply(\n",
    "    lambda x: rule_coverage(x, val_df), axis=1\n",
    ")\n",
    "\n",
    "# Calculate precision, recall, F1 score\n",
    "def calculate_precision_recall_f1(test_df, rules):\n",
    "    precision_numerator = 0  # ∑|R(u)∩T(u)|\n",
    "    precision_denominator = 0  # ∑|R(u)|\n",
    "    recall_denominator = 0  # ∑|T(u)|\n",
    "\n",
    "    for idx, row in test_df.iterrows():\n",
    "        # Extract user attributes (antecedent candidates)\n",
    "        gender = row['Gender']\n",
    "        income_tier = row['Income Tier']\n",
    "        region = row['Region']\n",
    "        time_on_site_tier = row['Time on Site Tier']\n",
    "        age_group = get_age_group(row['Age'])\n",
    "        user_attributes = {gender, income_tier, region, time_on_site_tier, age_group}\n",
    "\n",
    "        # Parse true purchase categories T(u)\n",
    "        purchase_history = json.loads(row['Purchase History'])\n",
    "        T_u = {item['Product Category'] for item in purchase_history}\n",
    "        recall_denominator += len(T_u)\n",
    "\n",
    "        # Generate recommendation list R(u): match all rules whose antecedent is a subset of user attributes\n",
    "        R_u = set()\n",
    "        for _, rule in rules.iterrows():\n",
    "            antecedent = set(rule['antecedents'])\n",
    "            if antecedent.issubset(user_attributes):\n",
    "                # Extract the unique element from frozenset using next(iter())\n",
    "                consequent_item = next(iter(rule['consequents']))\n",
    "                R_u.add(consequent_item)\n",
    "\n",
    "        precision_denominator += len(R_u)\n",
    "        intersection = R_u & T_u\n",
    "        precision_numerator += len(intersection)\n",
    "\n",
    "    precision = precision_numerator / precision_denominator if precision_denominator != 0 else 0\n",
    "    recall = precision_numerator / recall_denominator if recall_denominator != 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1\n",
    "    }\n",
    "\n",
    "test_df = pd.read_csv(r\"C:\\Users\\Dell\\Desktop\\E-commerce06.csv\")\n",
    "metrics = calculate_precision_recall_f1(test_df, final_rules)\n",
    "\n",
    "print(\"\\nRecommendation Performance Metrics:\")\n",
    "print(f\"Precision: {metrics['Precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['Recall']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['F1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba7f8ad-5361-47de-9643-1b98b01a821e",
   "metadata": {},
   "source": [
    "Association Analysis Part - Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360418a2-bd89-46ec-85ff-abad8b5bd384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Association Rule Visualization (Support vs Confidence with Lift as Bubble Size)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    data=final_rules,\n",
    "    x='support', y='confidence',\n",
    "    size='Lift', alpha=0.7,\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Association Rule Visualization (Support vs Confidence, Bubble Size Indicates Lift)')\n",
    "plt.legend(title='Lift', bbox_to_anchor=(1, 1))\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 3. Rule Consequent Distribution (Recommended Product Category Frequency)\n",
    "consequent_counts = final_rules['consequents'].apply(lambda x: next(iter(x))).value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=consequent_counts.values, y=consequent_counts.index, palette='plasma')\n",
    "plt.xlabel('Number of Rules')\n",
    "plt.ylabel('Recommended Product Categories')\n",
    "plt.title('Distribution of Recommended Product Categories')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display all charts\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
